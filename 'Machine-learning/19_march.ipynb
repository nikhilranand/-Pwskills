{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2ad7ac-1ea8-43a4-815b-3018fca06a1b",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588adb9-faab-4f7e-8d81-8643c287668b",
   "metadata": {},
   "source": [
    "**Min-Max Scaling** is a data preprocessing technique used to transform numerical features in a dataset so that they fall within a specific range, usually between 0 and 1. The purpose of Min-Max scaling is to bring all the features to a common scale, which can be especially useful when features have different ranges or units. This scaling ensures that the features have similar magnitudes, which can help certain machine learning algorithms converge faster and perform better.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "```\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "```\n",
    "Where:\n",
    "- `X` is the original feature value.\n",
    "- `X_min` is the minimum value of the feature.\n",
    "- `X_max` is the maximum value of the feature.\n",
    "\n",
    "This formula scales each feature value to a range between 0 and 1. If a feature has a value equal to its minimum value, it will be scaled to 0; if it has a value equal to its maximum value, it will be scaled to 1.\n",
    "\n",
    "**Example**:\n",
    "Let's consider a dataset with a single feature representing the age of individuals. The ages in the dataset range from 18 to 60 years. We want to apply Min-Max scaling to transform the ages to a range between 0 and 1.\n",
    "\n",
    "Original dataset:\n",
    "```\n",
    "Age: [18, 25, 35, 60]\n",
    "```\n",
    "\n",
    "1. Calculate the minimum and maximum values:\n",
    "   - `X_min = 18`\n",
    "   - `X_max = 60`\n",
    "\n",
    "2. Apply Min-Max scaling to each value:\n",
    "   - For age 18: `X_scaled = (18 - 18) / (60 - 18) = 0`\n",
    "   - For age 25: `X_scaled = (25 - 18) / (60 - 18) = 0.0952`\n",
    "   - For age 35: `X_scaled = (35 - 18) / (60 - 18) = 0.2857`\n",
    "   - For age 60: `X_scaled = (60 - 18) / (60 - 18) = 1`\n",
    "\n",
    "Scaled dataset:\n",
    "```\n",
    "Age_scaled: [0, 0.0952, 0.2857, 1]\n",
    "```\n",
    "\n",
    "In this example, Min-Max scaling has transformed the age values to a range between 0 and 1. Now the feature has consistent scaling, making it suitable for algorithms that are sensitive to feature scales, such as those involving distances or gradients.\n",
    "\n",
    "Remember that while Min-Max scaling is a useful technique, it might not be appropriate for all scenarios. For example, if your data has outliers, Min-Max scaling can be influenced by them. In such cases, you might consider other scaling techniques like Z-score normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080cddd-1b97-4960-985f-e0cd6996f4c6",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fde558-cc3b-4733-9881-0113eee22c1e",
   "metadata": {},
   "source": [
    "The **Unit Vector Scaling** technique, also known as **Vector Normalization**, is a feature scaling method that scales the feature vectors to have a length of 1 (unit length) while preserving the direction of the vector. It's commonly used in scenarios where the magnitude of the feature vectors matters more than their specific values. This technique is particularly useful in machine learning algorithms that rely on vector operations, such as cosine similarity or dot products.\n",
    "\n",
    "The formula for Unit Vector Scaling is as follows:\n",
    "```\n",
    "X_normalized = X / ||X||\n",
    "```\n",
    "Where:\n",
    "- `X` is the original feature vector.\n",
    "- `||X||` represents the Euclidean norm (length) of the feature vector `X`.\n",
    "\n",
    "In contrast to Min-Max scaling, which scales features to a specific range (e.g., between 0 and 1), Unit Vector Scaling ensures that the length of each feature vector becomes 1, while the direction of the vector remains the same.\n",
    "\n",
    "**Example**:\n",
    "Consider a dataset with two features, representing the height and weight of individuals. The goal is to apply Unit Vector Scaling to the feature vectors.\n",
    "\n",
    "Original dataset:\n",
    "```\n",
    "Height: [160, 175, 180]\n",
    "Weight: [50, 70, 75]\n",
    "```\n",
    "\n",
    "1. Calculate the length (Euclidean norm) of each feature vector:\n",
    "   - For the first individual: `||X|| = sqrt(160^2 + 50^2) ≈ 167.97`\n",
    "   - For the second individual: `||X|| = sqrt(175^2 + 70^2) ≈ 184.83`\n",
    "   - For the third individual: `||X|| = sqrt(180^2 + 75^2) ≈ 189.74`\n",
    "\n",
    "2. Apply Unit Vector Scaling to each feature vector:\n",
    "   - For the first individual: `X_normalized = [160/167.97, 50/167.97] ≈ [0.9526, 0.2988]`\n",
    "   - For the second individual: `X_normalized = [175/184.83, 70/184.83] ≈ [0.9469, 0.3212]`\n",
    "   - For the third individual: `X_normalized = [180/189.74, 75/189.74] ≈ [0.9481, 0.3941]`\n",
    "\n",
    "Scaled dataset:\n",
    "```\n",
    "Height_normalized: [0.9526, 0.9469, 0.9481]\n",
    "Weight_normalized: [0.2988, 0.3212, 0.3941]\n",
    "```\n",
    "\n",
    "In this example, Unit Vector Scaling has transformed the feature vectors to have unit lengths while preserving their directions. This technique is particularly useful when you want to emphasize the relative relationships between the features without being concerned about their specific magnitudes.\n",
    "\n",
    "It's important to note that Unit Vector Scaling is different from Min-Max scaling in terms of the transformation applied to the features. Min-Max scaling adjusts the values to a specific range, while Unit Vector Scaling maintains the direction of the vectors while ensuring they have unit lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23efd62-ebf1-4cae-bf2b-acf80d4002b7",
   "metadata": {},
   "source": [
    "## CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51752c-7ec3-4b3e-b83d-b83431d8e7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52e84d-2273-4a19-8e40-c14c131515ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15319c6-4f4e-4e52-8c11-178373d27dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ea15f-beaa-4052-968a-cf4c7684e3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7640400-c06b-4e2b-8a97-dd53a7ecd457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54e06fe5-86a0-47db-b66c-17faa9e9f644",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545e897-7c39-4daa-837b-3a359135beb9",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while retaining as much of the original data's variance as possible. PCA aims to find new orthogonal axes, known as principal components, along which the data exhibits the most variance. By projecting the data onto these principal components, PCA reduces the dimensionality of the dataset while preserving the most important information.\n",
    "\n",
    "The main steps of PCA are as follows:\n",
    "\n",
    "1. **Standardize Data**:\n",
    "   Standardize the features to have zero mean and unit variance. This step is crucial to ensure that features with different scales do not dominate the analysis.\n",
    "\n",
    "2. **Calculate Covariance Matrix**:\n",
    "   Calculate the covariance matrix of the standardized features. The covariance matrix captures the relationships and correlations between the features.\n",
    "\n",
    "3. **Calculate Eigenvectors and Eigenvalues**:\n",
    "   Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. **Sort Eigenvectors by Eigenvalues**:\n",
    "   Sort the eigenvectors in descending order of their corresponding eigenvalues. The eigenvectors with the highest eigenvalues capture the most variance in the data.\n",
    "\n",
    "5. **Select Principal Components**:\n",
    "   Choose a subset of the top-k eigenvectors to form the principal components. These principal components form a new basis for the data.\n",
    "\n",
    "6. **Project Data onto Principal Components**:\n",
    "   Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "**Example**:\n",
    "Let's consider a dataset with two features, \"height\" and \"weight,\" for a group of individuals. The goal is to use PCA to reduce the dimensionality of the data while preserving the most important information.\n",
    "\n",
    "Original dataset:\n",
    "```\n",
    "Height: [160, 175, 180]\n",
    "Weight: [50, 70, 75]\n",
    "```\n",
    "\n",
    "1. Standardize the data:\n",
    "   Calculate the mean and standard deviation for each feature and standardize the features.\n",
    "\n",
    "2. Calculate the covariance matrix:\n",
    "   ```\n",
    "   Covariance Matrix:\n",
    "   | 175.33  27.33 |\n",
    "   |  27.33  12.33 |\n",
    "   ```\n",
    "\n",
    "3. Calculate eigenvectors and eigenvalues:\n",
    "   Compute the eigenvectors and eigenvalues of the covariance matrix. Suppose we obtain the following results:\n",
    "   ```\n",
    "   Eigenvalues: [180.62, 7.05]\n",
    "   Eigenvectors: [0.98, 0.20; 0.20, -0.98]\n",
    "   ```\n",
    "\n",
    "4. Sort eigenvectors by eigenvalues:\n",
    "   Since the eigenvalue 180.62 is much larger than 7.05, the corresponding eigenvector [0.98, 0.20] captures the most variance.\n",
    "\n",
    "5. Select principal components:\n",
    "   Choose the eigenvector [0.98, 0.20] as the principal component.\n",
    "\n",
    "6. Project data onto principal component:\n",
    "   Project the original data onto the principal component to obtain the lower-dimensional representation.\n",
    "\n",
    "In this example, PCA reduces the dimensionality of the data from two features (height and weight) to a single principal component. The principal component captures the most significant information, and the projection of data points onto this component provides a compressed representation of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6112c7b-f758-436c-85fe-c2655ab659c7",
   "metadata": {},
   "source": [
    "## CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573d31b-c625-4a20-a9c1-730fc03dbd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd17d18-ae2c-4887-8288-26f046a50132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6bd064-950a-40b0-b50e-c9bc8d84fba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6860eec-64b4-4714-a729-de2caac57aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d35eb2-6865-4239-a0c9-fce2fb2e20c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b07326-a0fd-4120-ac82-a8f9cb407e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9428648-f751-46d9-b79d-c10a4a52a6c1",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d23520-d4b7-4360-a623-4814a9225896",
   "metadata": {},
   "source": [
    "**PCA (Principal Component Analysis)** and **Feature Extraction** are closely related concepts in machine learning and data analysis. PCA is a specific technique that can be used for feature extraction, which involves transforming the original features of a dataset into a new set of features that captures the most important information in the data. PCA achieves feature extraction by identifying the directions of maximum variance, known as principal components, and projecting the data onto these components.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "1. **Identify Principal Components**:\n",
    "   PCA identifies the eigenvectors (principal components) of the covariance matrix of the original features. These eigenvectors represent the directions in which the data exhibits the most variance.\n",
    "\n",
    "2. **Rank Eigenvectors by Eigenvalues**:\n",
    "   PCA ranks the eigenvectors by their corresponding eigenvalues. The eigenvectors with higher eigenvalues capture more of the data's variance and are considered more important.\n",
    "\n",
    "3. **Select Principal Components**:\n",
    "   To perform feature extraction, you can choose a subset of the top-k principal components. These selected components serve as the new features.\n",
    "\n",
    "4. **Project Data onto Principal Components**:\n",
    "   For each data point, PCA projects the original features onto the selected principal components. This projection creates a new set of features that represent the data in a lower-dimensional space.\n",
    "\n",
    "5. **Create Transformed Dataset**:\n",
    "   The transformed dataset contains the new features obtained by projecting the original data onto the selected principal components.\n",
    "\n",
    "**Example**:\n",
    "Consider a dataset with multiple features related to images of handwritten digits. Each data point represents an image, and each feature corresponds to a pixel value. The goal is to use PCA for feature extraction to represent the images with fewer features while preserving the important information.\n",
    "\n",
    "Original dataset:\n",
    "```\n",
    "Image 1: [0.1, 0.3, 0.5, ...]\n",
    "Image 2: [0.2, 0.4, 0.6, ...]\n",
    "...\n",
    "Image N: [0.3, 0.5, 0.7, ...]\n",
    "```\n",
    "\n",
    "1. Calculate Covariance Matrix:\n",
    "   Compute the covariance matrix of the pixel values in the images.\n",
    "\n",
    "2. Calculate Eigenvectors and Eigenvalues:\n",
    "   Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "3. Sort Eigenvectors by Eigenvalues:\n",
    "   Rank the eigenvectors by their eigenvalues in descending order.\n",
    "\n",
    "4. Select Principal Components:\n",
    "   Choose the top-k eigenvectors as the principal components for feature extraction.\n",
    "\n",
    "5. Project Data onto Principal Components:\n",
    "   For each image, project the original pixel values onto the selected principal components.\n",
    "\n",
    "6. Create Transformed Dataset:\n",
    "   The transformed dataset contains the new features, which are the projections of images onto the selected principal components.\n",
    "\n",
    "By applying PCA for feature extraction, the original pixel values are transformed into a lower-dimensional representation that captures the main patterns and variations present in the images. This can be particularly useful for reducing the computational complexity of algorithms or visualizing high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2620374-14d9-4878-851a-5ce6f5dfdca7",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bbfa6-eb11-42a8-ae8a-763bbe1ef390",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, you can use Min-Max scaling to preprocess the data before feeding it into your recommendation algorithm. Min-Max scaling will help ensure that the features have similar scales, which can lead to better performance of the recommendation system. Here's how you could use Min-Max scaling to preprocess the dataset:\n",
    "\n",
    "1. **Understand the Data**:\n",
    "   Begin by understanding the dataset and the features it contains. In this case, you mentioned features like price, rating, and delivery time.\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   Clean the dataset by handling missing values, outliers, and any other data quality issues. Ensure that the data is in a suitable format for analysis.\n",
    "\n",
    "3. **Feature Selection**:\n",
    "   Choose the relevant features that you want to include in your recommendation system. For this example, let's assume you'll be using price, rating, and delivery time.\n",
    "\n",
    "4. **Apply Min-Max Scaling**:\n",
    "   For each selected feature, apply Min-Max scaling to ensure that they fall within a specific range (typically between 0 and 1).\n",
    "\n",
    "   The formula for Min-Max scaling is:\n",
    "   ```\n",
    "   X_scaled = (X - X_min) / (X_max - X_min)\n",
    "   ```\n",
    "\n",
    "   - `X`: Original feature value.\n",
    "   - `X_min`: Minimum value of the feature.\n",
    "   - `X_max`: Maximum value of the feature.\n",
    "\n",
    "   Calculate the minimum and maximum values for each feature.\n",
    "\n",
    "5. **Perform Scaling**:\n",
    "   Apply the Min-Max scaling formula to each feature value to obtain the scaled values. This will ensure that all the features are transformed to a common range.\n",
    "\n",
    "6. **Updated Dataset**:\n",
    "   Replace the original feature values with the scaled values in the dataset.\n",
    "\n",
    "After applying Min-Max scaling, your dataset will have transformed feature values that fall within the 0 to 1 range. This normalization helps ensure that no single feature dominates the recommendations due to its larger scale compared to other features.\n",
    "\n",
    "For instance, if the original dataset looked like this:\n",
    "```\n",
    "Price: [5, 10, 15, 20]\n",
    "Rating: [3.5, 4.2, 4.8, 4.0]\n",
    "Delivery Time: [20, 30, 25, 40]\n",
    "```\n",
    "\n",
    "After Min-Max scaling, it might look like:\n",
    "```\n",
    "Price_scaled: [0.0, 0.333, 0.667, 1.0]\n",
    "Rating_scaled: [0.0, 0.6, 1.0, 0.2]\n",
    "Delivery Time_scaled: [0.2, 0.4, 0.3, 0.8]\n",
    "```\n",
    "\n",
    "Now you can use this scaled data as input to your recommendation algorithm to build a more balanced and effective recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb85376-d71b-4b86-8c9f-8a5cfdf986ec",
   "metadata": {},
   "source": [
    "## CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfb7ee-5407-4be0-9f15-421d75e6854b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113caeb6-a3cd-43f5-bbbe-42e8595e3458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b5aed-add6-4db1-bf53-5bf3215d750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab1bf2-9a7d-4121-ba54-7d00e86e26f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03b9dbf9-ff1b-487a-83f5-203bb2b67aca",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11937076-b0d8-46e1-b410-98e6918d3cb7",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset when building a model to predict stock prices involves transforming the original features into a lower-dimensional representation while retaining the most important information. This can help mitigate the curse of dimensionality, reduce noise, and improve the efficiency and performance of your stock price prediction model. Here's how you would use PCA for dimensionality reduction in the context of your project:\n",
    "\n",
    "1. **Understand the Dataset**:\n",
    "   Begin by understanding the features in your dataset. In this case, you mentioned company financial data and market trends as features.\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   Clean the dataset by handling missing values, outliers, and other data quality issues. Ensure that the data is in a suitable format for analysis.\n",
    "\n",
    "3. **Standardize Features**:\n",
    "   Standardize the features to have zero mean and unit variance. This is important to ensure that features with different scales do not dominate the PCA analysis.\n",
    "\n",
    "4. **Calculate Covariance Matrix**:\n",
    "   Calculate the covariance matrix of the standardized features. The covariance matrix captures the relationships and correlations between the features.\n",
    "\n",
    "5. **Calculate Eigenvectors and Eigenvalues**:\n",
    "   Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "6. **Sort Eigenvectors by Eigenvalues**:\n",
    "   Rank the eigenvectors by their corresponding eigenvalues in descending order. Eigenvectors with higher eigenvalues capture more variance and are considered more important.\n",
    "\n",
    "7. **Select Principal Components**:\n",
    "   Choose a subset of the top-k eigenvectors to form the principal components. These selected components will serve as the new features.\n",
    "\n",
    "8. **Project Data onto Principal Components**:\n",
    "   Project the original data onto the selected principal components to obtain a lower-dimensional representation.\n",
    "\n",
    "9. **Create Transformed Dataset**:\n",
    "   The transformed dataset will contain the new features, which are the projections of the original data onto the selected principal components.\n",
    "\n",
    "10. **Model Building and Training**:\n",
    "    Use the transformed dataset as input for your stock price prediction model. Train and validate the model using appropriate techniques.\n",
    "\n",
    "By applying PCA, you'll achieve a more compact representation of the original feature space. However, keep in mind that while PCA reduces dimensionality, it also means you're working with transformed features that might not have direct interpretability. Additionally, the effectiveness of PCA depends on the extent of correlation and variance in the original dataset.\n",
    "\n",
    "Overall, using PCA for dimensionality reduction can help improve the efficiency and performance of your stock price prediction model by focusing on the most relevant information and reducing noise caused by high dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fad25f-f0f5-4391-a543-8f8d9e479bcc",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686c64e-9388-47b2-be5e-9c3420be7c55",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling and transform the given values [1, 5, 10, 15, 20] to a range of -1 to 1, follow these steps:\n",
    "\n",
    "1. **Calculate Min and Max**:\n",
    "   Calculate the minimum and maximum values from the original dataset.\n",
    "   - `X_min = 1`\n",
    "   - `X_max = 20`\n",
    "\n",
    "2. **Apply Min-Max Scaling Formula**:\n",
    "   Apply the Min-Max scaling formula to each value using the given range [-1, 1]:\n",
    "   ```\n",
    "   X_scaled = -1 + 2 * (X - X_min) / (X_max - X_min)\n",
    "   ```\n",
    "\n",
    "3. **Perform Scaling**:\n",
    "   Apply the formula to each value to obtain the scaled values.\n",
    "\n",
    "Let's calculate the scaled values for each element:\n",
    "\n",
    "For `X = 1`:\n",
    "```\n",
    "X_scaled = -1 + 2 * (1 - 1) / (20 - 1) = -1 + 0 = -1\n",
    "```\n",
    "\n",
    "For `X = 5`:\n",
    "```\n",
    "X_scaled = -1 + 2 * (5 - 1) / (20 - 1) ≈ -0.6\n",
    "```\n",
    "\n",
    "For `X = 10`:\n",
    "```\n",
    "X_scaled = -1 + 2 * (10 - 1) / (20 - 1) ≈ -0.1\n",
    "```\n",
    "\n",
    "For `X = 15`:\n",
    "```\n",
    "X_scaled = -1 + 2 * (15 - 1) / (20 - 1) ≈ 0.4\n",
    "```\n",
    "\n",
    "For `X = 20`:\n",
    "```\n",
    "X_scaled = -1 + 2 * (20 - 1) / (20 - 1) = 1\n",
    "```\n",
    "\n",
    "The Min-Max scaled values are approximately: [-1, -0.6, -0.1, 0.4, 1]\n",
    "\n",
    "After performing Min-Max scaling, the original values [1, 5, 10, 15, 20] have been transformed to the desired range of -1 to 1. This scaling ensures that the values are uniformly spread within the specified range, which can be useful for certain algorithms or analyses that are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f492ee-14fa-41c6-8bed-95f7bea485c3",
   "metadata": {},
   "source": [
    "## CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af4a0d1-88ba-4e8e-8ab9-b05b55b52f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09e7235b-0ab7-4b23-b1b1-d049651420e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1,5,10,15,20]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf093c26-c83e-4407-b675-bf88535e1cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a6b6c07-5305-4f7a-8b3e-5aa37ac5edee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(feature_range=(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "print(scaler.fit(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68948844-ed8e-44ba-b955-780d98538bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(scaler.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef1533-548b-4552-a277-7616add807eb",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6681d4b-ffab-4afd-93d2-e1b435c8030c",
   "metadata": {},
   "source": [
    "The number of principal components to retain during feature extraction using PCA depends on the specific goals of your analysis, the nature of the dataset, and the trade-off between dimensionality reduction and preserving the information.\n",
    "\n",
    "Here's a general approach to determining the number of principal components to retain:\n",
    "\n",
    "1. **Explained Variance**:\n",
    "   One common method is to consider the cumulative explained variance. This measures how much of the total variance in the original data is captured by each principal component. You can plot the cumulative explained variance against the number of principal components and look for an \"elbow point\" where adding more components provides diminishing returns in terms of variance explained.\n",
    "\n",
    "2. **Retain a Sufficient Percentage of Variance**:\n",
    "   You might decide to retain a certain percentage of the total variance, such as 95% or 99%. This can be a more practical approach as it ensures that you're preserving most of the important information while reducing dimensionality.\n",
    "\n",
    "3. **Domain Knowledge**:\n",
    "   Consider the significance of the retained principal components in the context of your dataset and analysis. Some components might be more meaningful or relevant than others, and domain knowledge can guide your decision.\n",
    "\n",
    "4. **Model Performance**:\n",
    "   Retaining more principal components might lead to better model performance, but it can also introduce noise. It's important to balance dimensionality reduction with maintaining model interpretability and generalization.\n",
    "\n",
    "5. **Computational Constraints**:\n",
    "   The number of principal components might also be influenced by computational constraints. More components require more computation.\n",
    "\n",
    "Without specific information about the nature of your dataset and the goals of your analysis, it's challenging to give a definitive answer about the number of principal components to retain. However, I can provide a general example:\n",
    "\n",
    "Assume you have a dataset with the following features: height, weight, age, gender, and blood pressure. Let's say you want to retain 95% of the variance explained by the principal components. You can use the explained variance ratio obtained from PCA and select the number of components that achieves or comes close to the desired explained variance threshold.\n",
    "\n",
    "For example, if you find that the first 3 principal components together explain 90% of the variance, and adding the fourth component increases the explained variance to 95%, you might choose to retain the first 4 principal components.\n",
    "\n",
    "Remember that the choice of the number of principal components involves a balance between capturing sufficient information and reducing dimensionality. It's often a good practice to experiment with different numbers and evaluate their impact on model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb003d7-fec9-4d1c-b132-9b94a0991aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
